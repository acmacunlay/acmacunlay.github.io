{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"What's New","text":"<ul> <li>\u2728 New Article: Managing Amazon S3 Buckets and Objects using Boto3</li> <li>\ud83d\udcdd Updated Article: Python Logging Part 2: AWS CloudWatch Logs Handler</li> <li>\ud83d\udcdd Updated Article: Python Logging Part 1: Handlers and Formatters</li> </ul>"},{"location":"#about-this-site","title":"About this Site","text":"<p>The motivation behind launching this website stems from my interest in personal skill development and the desire to share valuable knowledge I gain. This website offers an opportunity to disseminate insights and information, providing a space for others to learn and grow. Through the act of sharing, I aim to contribute positively to the broader knowledge base while furthering my own journey of continuous improvement.</p>"},{"location":"tags/","title":"Tags","text":"<p>Following is a list of relevant tags:</p>"},{"location":"tags/#aws","title":"AWS","text":"<ul> <li>Managing Amazon S3 Buckets and Objects using Boto3</li> <li>Python Logging Part 2: AWS CloudWatch Logs Handler</li> </ul>"},{"location":"tags/#python","title":"Python","text":"<ul> <li>Managing Amazon S3 Buckets and Objects using Boto3</li> <li>Python Logging Part 1: Handlers and Formatters</li> <li>Python Logging Part 2: AWS CloudWatch Logs Handler</li> </ul>"},{"location":"tags/#software-development","title":"Software Development","text":"<ul> <li>Managing Amazon S3 Buckets and Objects using Boto3</li> <li>Python Logging Part 1: Handlers and Formatters</li> <li>Python Logging Part 2: AWS CloudWatch Logs Handler</li> </ul>"},{"location":"blog/","title":"Articles","text":""},{"location":"blog/524ea484-523b-4e98-ab37-bbbdb48af016/","title":"Managing Amazon S3 Buckets and Objects using Boto3","text":"<p>Amazon S3 (Simple Storage Service) is a pivotal component in cloud computing and data management. This article provides a practical guide on effectively utilizing Boto3, the Amazon Web Services (AWS) SDK for Python, to efficiently manage S3 buckets. Whether you're new to AWS or looking to streamline your bucket management, this straightforward tutorial will help you harness the power of Boto3 for seamless S3 bucket operations.</p> Changelog <ul> <li>2023-10-29<ul> <li>initially published</li> </ul> </li> </ul>","tags":["AWS","Python","Software Development"]},{"location":"blog/524ea484-523b-4e98-ab37-bbbdb48af016/#prerequisites","title":"Prerequisites","text":"<p>Before proceeding with the next sections, you must ensure that you have:</p> <ol> <li>An active AWS Account</li> <li>A working installation of AWS CLI v2</li> <li>Configured AWS credentials (see instructions)</li> </ol> <p>Tip</p> <p>In a hurry? You can skip to the Final Code to get all working implementations of all operations to be discussed in Implementation and Usage.</p>","tags":["AWS","Python","Software Development"]},{"location":"blog/524ea484-523b-4e98-ab37-bbbdb48af016/#project-structure","title":"Project Structure","text":"<p>The project folder we will be working on will have a structure as shown below:</p> <pre><code>aws_s3/__init__.py # (1)!\naws_s3/api.py # (2)!\napp.py # (3)!\nrequirements.txt # (4)!\n</code></pre> <ol> <li>All functions that is publicly accessible from <code>aws_s3/api.py</code> will be imported here.</li> <li>All the implementation logic for managing S3 buckets and objects will be written here.</li> <li>All usage samples will be written here.</li> <li>This <code>.txt</code> file will contain all dependencies needed for the AWS SDK client to work.</li> </ol>","tags":["AWS","Python","Software Development"]},{"location":"blog/524ea484-523b-4e98-ab37-bbbdb48af016/#dependencies","title":"Dependencies","text":"<p>The following external dependencies will be used throughout the article:</p> requirements.txt<pre><code>boto3==1.28.63\nmypy-boto3-s3==1.28.55\nrich\n</code></pre> <p>Note</p> <p>I'll be using the <code>rich</code> library for printing the outputs in the console for better output formatting. You can check out this project here.</p>","tags":["AWS","Python","Software Development"]},{"location":"blog/524ea484-523b-4e98-ab37-bbbdb48af016/#implementation-and-usage","title":"Implementation and Usage","text":"","tags":["AWS","Python","Software Development"]},{"location":"blog/524ea484-523b-4e98-ab37-bbbdb48af016/#client-and-exceptions","title":"Client and Exceptions","text":"<p>Creating a Boto3 client for Amazon S3 and handling client exceptions in Python is relatively straightforward. Boto3 provides a convenient way to interact with AWS services, including S3. Creating an S3 client and handling exceptions can be done as shown below:</p> aws_s3/api.pyaws_s3/__init__.pyapp.py <p>Tip</p> <p>The snippet below is annotated to explain blocks of code inline.</p> <pre><code>from __future__ import annotations\n\nimport functools\nfrom typing import Tuple, Type\n\nimport boto3 as aws_sdk\nfrom mypy_boto3_s3 import S3Client\nfrom mypy_boto3_s3.client import BotocoreClientError\n\n\n@functools.lru_cache(maxsize=2**0) # (1)!\ndef __get_client() -&gt; S3Client:\n    session = aws_sdk.Session()\n    return session.client(\"s3\")\n\n\n@functools.lru_cache(maxsize=2**0) # (2)!\ndef __get_exceptions() -&gt; Tuple[Type[BotocoreClientError], ...]:\n    client = __get_client()\n    return (\n        client.exceptions.BucketAlreadyExists,\n        client.exceptions.BucketAlreadyOwnedByYou,\n        client.exceptions.ClientError,\n        client.exceptions.InvalidObjectState,\n        client.exceptions.NoSuchBucket,\n        client.exceptions.NoSuchKey,\n        client.exceptions.NoSuchUpload,\n        client.exceptions.ObjectAlreadyInActiveTierError,\n        client.exceptions.ObjectNotInActiveTierError,\n    )\n</code></pre> <ol> <li>Caching the results of this function will improve performace by creating the instance of the client only once.</li> <li>Same with caching the instantiated client, this will improve performance during exception handling since all error handling will be referring to the already created tuple of exceptions.</li> </ol> <p>This is not applicable since the client and exceptions will not be available in the public scope.</p> <p>This is not applicable since the client and exceptions will not be available in the public scope.</p>","tags":["AWS","Python","Software Development"]},{"location":"blog/524ea484-523b-4e98-ab37-bbbdb48af016/#creating-a-bucket","title":"Creating a Bucket","text":"<p>Creating an Amazon S3 bucket is a straightforward process. First, specify a unique bucket name that adheres to naming rules and provide optional configuration details like the AWS region in which the bucket should be created. After defining the parameters, use the <code>create_bucket</code> method on the S3 client to create the bucket. It's essential to handle exceptions gracefully, as bucket names must be globally unique, and there can be issues with permissions or existing buckets. Once executed, you'll have successfully created an S3 bucket, ready to store your objects and data securely in the cloud. A sample implementation and usage is shown below:</p> aws_s3/api.pyaws_s3/__init__.pyapp.py <pre><code>from __future__ import annotations\n\nimport functools\nfrom typing import Any, Dict, Optional, Tuple, Type, Union\n\nimport boto3 as aws_sdk\nfrom mypy_boto3_s3 import S3Client\nfrom mypy_boto3_s3.client import BotocoreClientError\nfrom mypy_boto3_s3.type_defs import (\n    CreateBucketConfigurationTypeDef,\n    CreateBucketOutputTypeDef,\n)\n\n\n@functools.lru_cache(maxsize=2**0)\ndef __get_client() -&gt; S3Client:\n    session = aws_sdk.Session()\n    return session.client(\"s3\")\n\n\n@functools.lru_cache(maxsize=2**0)\ndef __get_exceptions() -&gt; Tuple[Type[BotocoreClientError], ...]:\n    client = __get_client()\n    return (\n        client.exceptions.BucketAlreadyExists,\n        client.exceptions.BucketAlreadyOwnedByYou,\n        client.exceptions.ClientError,\n        client.exceptions.InvalidObjectState,\n        client.exceptions.NoSuchBucket,\n        client.exceptions.NoSuchKey,\n        client.exceptions.NoSuchUpload,\n        client.exceptions.ObjectAlreadyInActiveTierError,\n        client.exceptions.ObjectNotInActiveTierError,\n    )\n\n\ndef create_bucket(\n    bucket: str, region: Optional[str] = None\n) -&gt; Union[CreateBucketOutputTypeDef, Dict[str, Any]]:\n    \"\"\"\n    Create an AWS S3 bucket with name `bucket` in region `region`. If `region` is not\n    defined, then `region` will be set to `us-east-1`.\n    \"\"\"\n    try:\n        client = __get_client()\n        return client.create_bucket(\n            Bucket=bucket,\n            CreateBucketConfiguration=CreateBucketConfigurationTypeDef(\n                LocationConstraint=region or \"us-east-1\"\n            ),\n        )\n\n    except __get_exceptions() as e:\n        return e.response\n</code></pre> <pre><code>from .api import create_bucket\n</code></pre> <pre><code>from rich.pretty import pprint\n\nimport aws_s3\n\nbucket: str = \"my-bucket-00000001\"\nregion: str = \"ap-southeast-1\"\n\nresult = aws_s3.create_bucket(bucket, region)\n\npprint(result, indent_guides=False)\n</code></pre> <p>Output:</p> <pre><code>{\n    \"ResponseMetadata\": {\n        \"RequestId\": \"****************\",\n        \"HostId\": \"****************\",\n        \"HTTPStatusCode\": 200,\n        \"HTTPHeaders\": {\n            \"x-amz-id-2\": \"****************\",\n            \"x-amz-request-id\": \"****************\",\n            \"date\": \"Sun, 29 Oct 2023 08:32:25 GMT\",\n            \"location\": \"http://my-bucket-00000001.s3.amazonaws.com/\",\n            \"server\": \"AmazonS3\",\n            \"content-length\": \"0\",\n        },\n        \"RetryAttempts\": 0,\n    },\n    \"Location\": \"http://my-bucket-00000001.s3.amazonaws.com/\",\n}\n</code></pre>","tags":["AWS","Python","Software Development"]},{"location":"blog/524ea484-523b-4e98-ab37-bbbdb48af016/#getting-a-buckets-metadata","title":"Getting a Bucket's Metadata","text":"<p>To obtain the header information, you can use the <code>head_bucket</code> method of the S3 client, passing the bucket name as a parameter. This operation provides essential metadata about the bucket, including information on the bucket's location, creation date, and other configuration details. By following these steps, you can easily access and inspect header data for your S3 bucket, which is helpful for monitoring and managing your storage resources in the AWS cloud. A sample implementation and usage is shown below:</p> aws_s3/api.pyaws_s3/__init__.pyapp.py <pre><code>from __future__ import annotations\n\nimport functools\nfrom typing import Any, Dict, Tuple, Type, Union\n\nimport boto3 as aws_sdk\nfrom mypy_boto3_s3 import S3Client\nfrom mypy_boto3_s3.client import BotocoreClientError\nfrom mypy_boto3_s3.type_defs import EmptyResponseMetadataTypeDef\n\n\n@functools.lru_cache(maxsize=2**0)\ndef __get_client() -&gt; S3Client:\n    session = aws_sdk.Session()\n    return session.client(\"s3\")\n\n\n@functools.lru_cache(maxsize=2**0)\ndef __get_exceptions() -&gt; Tuple[Type[BotocoreClientError], ...]:\n    client = __get_client()\n    return (\n        client.exceptions.BucketAlreadyExists,\n        client.exceptions.BucketAlreadyOwnedByYou,\n        client.exceptions.ClientError,\n        client.exceptions.InvalidObjectState,\n        client.exceptions.NoSuchBucket,\n        client.exceptions.NoSuchKey,\n        client.exceptions.NoSuchUpload,\n        client.exceptions.ObjectAlreadyInActiveTierError,\n        client.exceptions.ObjectNotInActiveTierError,\n    )\n\n\ndef get_bucket_metadata(\n    bucket: str,\n) -&gt; Union[EmptyResponseMetadataTypeDef, Dict[str, Any]]:\n    try:\n        client = __get_client()\n        return client.head_bucket(Bucket=bucket)\n\n    except __get_exceptions() as e:\n        return e.response\n</code></pre> <pre><code>from .api import get_bucket_metadata\n</code></pre> <pre><code>from rich.pretty import pprint\n\nimport aws_s3\n\nbucket: str = \"my-bucket-00000001\"\n\nresult = aws_s3.get_bucket_metadata(bucket)\n\npprint(result, indent_guides=False)\n</code></pre> <p>Output:</p> <pre><code>{\n    \"ResponseMetadata\": {\n        \"RequestId\": \"****************\",\n        \"HostId\": \"****************\",\n        \"HTTPStatusCode\": 200,\n        \"HTTPHeaders\": {\n            \"x-amz-id-2\": \"****************\",\n            \"x-amz-request-id\": \"****************\",\n            \"date\": \"Sun, 29 Oct 2023 08:35:53 GMT\",\n            \"x-amz-bucket-region\": \"ap-southeast-1\",\n            \"x-amz-access-point-alias\": \"false\",\n            \"content-type\": \"application/xml\",\n            \"server\": \"AmazonS3\",\n        },\n        \"RetryAttempts\": 0,\n    }\n}\n</code></pre>","tags":["AWS","Python","Software Development"]},{"location":"blog/524ea484-523b-4e98-ab37-bbbdb48af016/#list-all-available-buckets","title":"List All Available Buckets","text":"<p>To list all available S3 buckets in your account, you can use the <code>list_buckets</code> method provided by the S3 client to fetch a list of all the buckets associated with your AWS account. The response will include details such as each bucket's name, creation date, and other metadata. This information can be invaluable for managing and monitoring your S3 resources, and it allows you to easily view and access your existing buckets, making it a fundamental step for AWS users to gain visibility into their S3 infrastructure. A sample implementation and usage is shown below:</p> aws_s3/api.pyaws_s3/__init__.pyapp.py <pre><code>from __future__ import annotations\n\nimport functools\nfrom typing import Any, Dict, Tuple, Type, Union\n\nimport boto3 as aws_sdk\nfrom mypy_boto3_s3 import S3Client\nfrom mypy_boto3_s3.client import BotocoreClientError\nfrom mypy_boto3_s3.type_defs import ListBucketsOutputTypeDef\n\n\n@functools.lru_cache(maxsize=2**0)\ndef __get_client() -&gt; S3Client:\n    session = aws_sdk.Session()\n    return session.client(\"s3\")\n\n\n@functools.lru_cache(maxsize=2**0)\ndef __get_exceptions() -&gt; Tuple[Type[BotocoreClientError], ...]:\n    client = __get_client()\n    return (\n        client.exceptions.BucketAlreadyExists,\n        client.exceptions.BucketAlreadyOwnedByYou,\n        client.exceptions.ClientError,\n        client.exceptions.InvalidObjectState,\n        client.exceptions.NoSuchBucket,\n        client.exceptions.NoSuchKey,\n        client.exceptions.NoSuchUpload,\n        client.exceptions.ObjectAlreadyInActiveTierError,\n        client.exceptions.ObjectNotInActiveTierError,\n    )\n\n\ndef list_buckets() -&gt; Union[ListBucketsOutputTypeDef, Dict[str, Any]]:\n    try:\n        client = __get_client()\n        return client.list_buckets()\n\n    except __get_exceptions() as e:\n        return e.response\n</code></pre> <pre><code>from .api import list_buckets\n</code></pre> <pre><code>from rich.pretty import pprint\n\nimport aws_s3\n\nresult = aws_s3.list_buckets()\n\npprint(result, indent_guides=False)\n</code></pre> <p>Output:</p> <pre><code>{\n    \"ResponseMetadata\": {\n        \"RequestId\": \"****************\",\n        \"HostId\": \"****************\",\n        \"HTTPStatusCode\": 200,\n        \"HTTPHeaders\": {\n            \"x-amz-id-2\": \"****************\",\n            \"x-amz-request-id\": \"****************\",\n            \"date\": \"Sun, 29 Oct 2023 08:40:29 GMT\",\n            \"content-type\": \"application/xml\",\n            \"transfer-encoding\": \"chunked\",\n            \"server\": \"AmazonS3\",\n        },\n        \"RetryAttempts\": 0,\n    },\n    \"Buckets\": [\n        {\n            \"Name\": \"****************\",\n            \"CreationDate\": datetime.datetime(2023, 5, 8, 14, 3, 23, tzinfo=tzutc()),\n        },\n        {\n            \"Name\": \"****************\",\n            \"CreationDate\": datetime.datetime(2023, 10, 16, 14, 44, 39, tzinfo=tzutc()),\n        },\n        {\n            \"Name\": \"my-bucket-00000001\",\n            \"CreationDate\": datetime.datetime(2023, 10, 29, 8, 32, 26, tzinfo=tzutc()),\n        },\n    ],\n    \"Owner\": {\"DisplayName\": \"****************\", \"ID\": \"****************\"},\n}\n</code></pre>","tags":["AWS","Python","Software Development"]},{"location":"blog/524ea484-523b-4e98-ab37-bbbdb48af016/#deleting-a-bucket","title":"Deleting a Bucket","text":"<p>To delete an s3 bucket, make use of the <code>delete_bucket</code> method provided by the S3 client, passing the name of the target bucket as a parameter. It's crucial to note that the bucket must be empty before you can delete it; otherwise, you'll encounter an error. A sample implementation and usage is shown below:</p> aws_s3/api.pyaws_s3/__init__.pyapp.py <pre><code>from __future__ import annotations\n\nimport functools\nfrom typing import Any, Dict, Tuple, Type\n\nimport boto3 as aws_sdk\nfrom mypy_boto3_s3 import S3Client\nfrom mypy_boto3_s3.client import BotocoreClientError\n\n\n@functools.lru_cache(maxsize=2**0)\ndef __get_client() -&gt; S3Client:\n    session = aws_sdk.Session()\n    return session.client(\"s3\")\n\n\n@functools.lru_cache(maxsize=2**0)\ndef __get_exceptions() -&gt; Tuple[Type[BotocoreClientError], ...]:\n    client = __get_client()\n    return (\n        client.exceptions.BucketAlreadyExists,\n        client.exceptions.BucketAlreadyOwnedByYou,\n        client.exceptions.ClientError,\n        client.exceptions.InvalidObjectState,\n        client.exceptions.NoSuchBucket,\n        client.exceptions.NoSuchKey,\n        client.exceptions.NoSuchUpload,\n        client.exceptions.ObjectAlreadyInActiveTierError,\n        client.exceptions.ObjectNotInActiveTierError,\n    )\n\n\ndef delete_bucket(bucket: str) -&gt; Dict[str, Any]:\n    try:\n        client = __get_client()\n        return client.delete_bucket(Bucket=bucket)\n\n    except __get_exceptions() as e:\n        return e.response\n</code></pre> <pre><code>from .api import delete_bucket\n</code></pre> <pre><code>from rich.pretty import pprint\n\nimport aws_s3\n\nbucket: str = \"my-bucket-00000001\"\n\nresult = aws_s3.delete_bucket(bucket)\n\npprint(result, indent_guides=False)\n</code></pre> <p>Output:</p> <pre><code>{\n    \"ResponseMetadata\": {\n        \"RequestId\": \"****************\",\n        \"HostId\": \"****************\",\n        \"HTTPStatusCode\": 204,\n        \"HTTPHeaders\": {\n            \"x-amz-id-2\": \"****************\",\n            \"x-amz-request-id\": \"****************\",\n            \"date\": \"Sun, 29 Oct 2023 08:44:33 GMT\",\n            \"server\": \"AmazonS3\",\n        },\n        \"RetryAttempts\": 0,\n    }\n}\n</code></pre>","tags":["AWS","Python","Software Development"]},{"location":"blog/524ea484-523b-4e98-ab37-bbbdb48af016/#fileobject-crud-operations-using-presigned-urls","title":"File/Object CRUD Operations Using Presigned URLs","text":"<p>Creating presigned URLs for S3 objects using Boto3 is essential when you need to grant temporary, controlled access to your objects. To generate a presigned URL, specify the S3 bucket name and the object's key (the filename or identifier). Using the <code>generate_presigned_url</code> method on the S3 client, you can set the expiration time for the URL and any additional HTTP request parameters, such as downloading or uploading options. Once generated, the presigned URL provides secure, time-limited access to the specified object, which can be useful for sharing files, enabling temporary public access, or granting controlled access to specific users or applications without exposing your S3 objects publicly. For most cases, managing s3 objects using presigned URLs is the simplest and most secure way to do so. A sample implementation and usage is shown below:</p> <p>Note</p> <p>The usage samples assume that you have an empty <code>.json</code> file in the project's root directory named <code>sample.json</code> with the content:</p> sample.json<pre><code>{\n    \"sample\": \"message\"\n}\n</code></pre> aws_s3/api.pyaws_s3/__init__.pyapp.py (<code>PUT</code>)app.py (<code>HEAD</code>)app.py (<code>GET</code>)app.py (<code>DELETE</code>) <pre><code>from __future__ import annotations\n\nimport functools\nfrom typing import Any, Dict, Literal, Tuple, Type, Union\n\nimport boto3 as aws_sdk\nfrom mypy_boto3_s3 import S3Client\nfrom mypy_boto3_s3.client import BotocoreClientError\n\n\n@functools.lru_cache(maxsize=2**0)\ndef __get_client() -&gt; S3Client:\n    session = aws_sdk.Session()\n    return session.client(\"s3\")\n\n\n@functools.lru_cache(maxsize=2**0)\ndef __get_exceptions() -&gt; Tuple[Type[BotocoreClientError], ...]:\n    client = __get_client()\n    return (\n        client.exceptions.BucketAlreadyExists,\n        client.exceptions.BucketAlreadyOwnedByYou,\n        client.exceptions.ClientError,\n        client.exceptions.InvalidObjectState,\n        client.exceptions.NoSuchBucket,\n        client.exceptions.NoSuchKey,\n        client.exceptions.NoSuchUpload,\n        client.exceptions.ObjectAlreadyInActiveTierError,\n        client.exceptions.ObjectNotInActiveTierError,\n    )\n\n\nHTTPVerb = Literal[\"GET\", \"PUT\", \"DELETE\", \"HEAD\"]\n\n\ndef generate_presigned_url(\n    method: HTTPVerb, bucket: str, key: str, *, ttl: int = 3600\n) -&gt; Union[str, Dict[str, Any]]:\n    try:\n        client = __get_client()\n        CLIENT_METHOD_DISPATCH: Dict[HTTPVerb, str] = {\n            \"GET\": client.get_object.__name__,\n            \"PUT\": client.put_object.__name__,\n            \"DELETE\": client.delete_object.__name__,\n            \"HEAD\": client.head_object.__name__,\n        }\n        return client.generate_presigned_url(\n            ClientMethod=CLIENT_METHOD_DISPATCH[method],\n            Params={\"Bucket\": bucket, \"Key\": key},\n            ExpiresIn=ttl,\n        )\n\n    except __get_exceptions() as e:\n        return e.response\n</code></pre> <pre><code>from .api import generate_presigned_url\n</code></pre> <pre><code>import requests\nfrom rich import print\n\nimport aws_s3\n\nbucket: str = \"my-bucket-00000001\"\nfile_path: str = \"sample.json\"\nkey: str = \"uploaded-sample.json\"\n\nwith open(file_path, \"rb\") as buffer:\n    url = aws_s3.generate_presigned_url(\"PUT\", bucket, key)\n    print(\"Generated URL: {}\".format(url))\n    response = requests.put(url, data=buffer.read())\n    print(response)\n</code></pre> <pre><code>import requests\nfrom rich import print\nfrom rich.pretty import pprint\n\nimport aws_s3\n\nbucket: str = \"my-bucket-00000001\"\nfile_path: str = \"sample.json\"\nkey: str = \"uploaded-sample.json\"\n\nwith open(file_path, \"rb\") as buffer:\n    url = aws_s3.generate_presigned_url(\"HEAD\", bucket, key)\n    print(\"Generated URL: {}\".format(url))\n\n    # This request will get the object's/file's URL\n    response1 = requests.head(url)\n    pprint(response1)\n    pprint(response1.headers)\n    object_url = response1.headers[\"Location\"]\n    print(\"Object URL: {}\".format(object_url))\n\n    # This request will get the object's/file's actual metadata\n    response2 = requests.head(object_url)\n    pprint(response2)\n    pprint(response2.headers)\n</code></pre> <pre><code>import requests\nfrom rich import print\n\nimport aws_s3\n\nbucket: str = \"my-bucket-00000001\"\nfile_path: str = \"sample.json\"\nkey: str = \"uploaded-sample.json\"\n\nwith open(file_path, \"wb\") as buffer:\n    url = aws_s3.generate_presigned_url(\"GET\", bucket, key)\n    print(\"Generated URL: {}\".format(url))\n\n    response = requests.get(url)\n    print(response)\n    buffer.write(response.content)\n</code></pre> <pre><code>import requests\nfrom rich import print\n\nimport aws_s3\n\nbucket: str = \"my-bucket-00000001\"\nfile_path: str = \"sample.json\"\nkey: str = \"uploaded-sample.json\"\n\nurl = aws_s3.generate_presigned_url(\"DELETE\", bucket, key)\nprint(\"Generated URL: {}\".format(url))\n\nresponse = requests.delete(url)\nprint(response)\n</code></pre>","tags":["AWS","Python","Software Development"]},{"location":"blog/524ea484-523b-4e98-ab37-bbbdb48af016/#getting-fileobject-metadata","title":"Getting File/Object Metadata","text":"<p>To get an S3 object's/file's metadata, use the <code>head_object</code> method provided by the S3 client, specifying the bucket name and object key. This operation retrieves metadata about the object, including details like content type, content length, last modified date, and more. This approach is useful for quickly inspecting object attributes and making decisions based on object properties, all without incurring the download cost or transferring the entire file, making it an efficient way to manage and work with your S3 objects. A sample implementation and usage is shown below:</p> aws_s3/api.pyaws_s3/__init__.pyapp.py <pre><code>from __future__ import annotations\n\nimport functools\nfrom typing import Any, Dict, Tuple, Type, Union\n\nimport boto3 as aws_sdk\nfrom mypy_boto3_s3 import S3Client\nfrom mypy_boto3_s3.client import BotocoreClientError\nfrom mypy_boto3_s3.type_defs import HeadObjectOutputTypeDef\n\n\n@functools.lru_cache(maxsize=2**0)\ndef __get_client() -&gt; S3Client:\n    session = aws_sdk.Session()\n    return session.client(\"s3\")\n\n\n@functools.lru_cache(maxsize=2**0)\ndef __get_exceptions() -&gt; Tuple[Type[BotocoreClientError], ...]:\n    client = __get_client()\n    return (\n        client.exceptions.BucketAlreadyExists,\n        client.exceptions.BucketAlreadyOwnedByYou,\n        client.exceptions.ClientError,\n        client.exceptions.InvalidObjectState,\n        client.exceptions.NoSuchBucket,\n        client.exceptions.NoSuchKey,\n        client.exceptions.NoSuchUpload,\n        client.exceptions.ObjectAlreadyInActiveTierError,\n        client.exceptions.ObjectNotInActiveTierError,\n    )\n\n\n@functools.lru_cache(maxsize=2**16)\ndef get_object_metadata(\n    bucket: str, key: str\n) -&gt; Union[HeadObjectOutputTypeDef, Dict[str, Any]]:\n    try:\n        client = __get_client()\n        return client.head_object(Bucket=bucket, Key=key)\n\n    except __get_exceptions() as e:\n        return e.response\n</code></pre> <pre><code>from .api import get_object_metadata\n</code></pre> <pre><code>from rich.pretty import pprint\n\nimport aws_s3\n\nbucket: str = \"my-bucket-00000001\"\nkey: str = \"uploaded-sample.json\"\n\nresult = aws_s3.get_object_metadata(bucket, key)\npprint(result)\n</code></pre> <p>Output:</p> <pre><code>{\n    \"ResponseMetadata\": {\n        \"RequestId\": \"****************\",\n        \"HostId\": \"****************\",\n        \"HTTPStatusCode\": 200,\n        \"HTTPHeaders\": {\n            \"x-amz-id-2\": \"****************\",\n            \"x-amz-request-id\": \"****************\",\n            \"date\": \"Sun, 29 Oct 2023 14:54:18 GMT\",\n            \"last-modified\": \"Sun, 29 Oct 2023 13:47:57 GMT\",\n            \"etag\": '\"5bd9fe559a5630150c700f98bf10fe79\"',\n            \"x-amz-server-side-encryption\": \"AES256\",\n            \"accept-ranges\": \"bytes\",\n            \"content-type\": \"binary/octet-stream\",\n            \"server\": \"AmazonS3\",\n            \"content-length\": \"28\",\n        },\n        \"RetryAttempts\": 0,\n    },\n    \"AcceptRanges\": \"bytes\",\n    \"LastModified\": datetime.datetime(2023, 10, 29, 13, 47, 57, tzinfo=tzutc()),\n    \"ContentLength\": 28,\n    \"ETag\": '\"5bd9fe559a5630150c700f98bf10fe79\"',\n    \"ContentType\": \"binary/octet-stream\",\n    \"ServerSideEncryption\": \"AES256\",\n    \"Metadata\": {},\n}\n</code></pre>","tags":["AWS","Python","Software Development"]},{"location":"blog/524ea484-523b-4e98-ab37-bbbdb48af016/#listing-multiple-objects-in-a-bucket","title":"Listing Multiple Objects in a Bucket","text":"<p>To list objects in an S3 bucket, use the <code>list_objects</code> method, specifying the bucket name and optional parameters to filter or paginate results. This operation returns a list of objects within the specified bucket, including their keys, sizes, and other metadata.  A sample implementation and usage is shown below:</p> aws_s3/api.pyaws_s3/__init__.pyapp.py <pre><code>from __future__ import annotations\n\nimport functools\nfrom typing import Any, Dict, List, Optional, Tuple, Type, Union, cast\n\nimport boto3 as aws_sdk\nfrom mypy_boto3_s3 import S3Client\nfrom mypy_boto3_s3.client import BotocoreClientError\nfrom mypy_boto3_s3.paginator import ListObjectsV2Paginator\nfrom mypy_boto3_s3.type_defs import ObjectTypeDef\n\n\n@functools.lru_cache(maxsize=2**0)\ndef __get_client() -&gt; S3Client:\n    session = aws_sdk.Session()\n    return session.client(\"s3\")\n\n\n@functools.lru_cache(maxsize=2**0)\ndef __get_exceptions() -&gt; Tuple[Type[BotocoreClientError], ...]:\n    client = __get_client()\n    return (\n        client.exceptions.BucketAlreadyExists,\n        client.exceptions.BucketAlreadyOwnedByYou,\n        client.exceptions.ClientError,\n        client.exceptions.InvalidObjectState,\n        client.exceptions.NoSuchBucket,\n        client.exceptions.NoSuchKey,\n        client.exceptions.NoSuchUpload,\n        client.exceptions.ObjectAlreadyInActiveTierError,\n        client.exceptions.ObjectNotInActiveTierError,\n    )\n\n\ndef list_objects(\n    bucket: str, prefix: Optional[str] = None\n) -&gt; Union[List[ObjectTypeDef], Dict[str, Any]]:\n    \"\"\"\n    Reference/s:\n        - [StackOverflow](https://stackoverflow.com/a/59816089)\n    \"\"\"\n    try:\n        client = __get_client()\n        paginator = cast(\n            ListObjectsV2Paginator,\n            client.get_paginator(client.list_objects_v2.__name__),\n        )\n\n        result: List[ObjectTypeDef] = []\n        for each_page in paginator.paginate(Bucket=bucket, Prefix=prefix or \"\"):\n            result.extend(each_page[\"Contents\"])\n\n        return result\n\n    except __get_exceptions() as e:\n        return e.response\n</code></pre> <pre><code>from .api import list_objects\n</code></pre> <pre><code>from rich.pretty import pprint\n\nimport aws_s3\n\nbucket: str = \"my-bucket-00000001\"\nresult = aws_s3.list_objects(bucket)\npprint(result, indent_guides=False)\n</code></pre> <p>Output:</p> <pre><code>[\n    {\n        \"Key\": \"uploaded-sample.json\",\n        \"LastModified\": datetime.datetime(2023, 10, 29, 13, 47, 57, tzinfo=tzutc()),\n        \"ETag\": '\"5bd9fe559a5630150c700f98bf10fe79\"',\n        \"Size\": 28,\n        \"StorageClass\": \"STANDARD\",\n    }\n]\n</code></pre>","tags":["AWS","Python","Software Development"]},{"location":"blog/524ea484-523b-4e98-ab37-bbbdb48af016/#uploading-a-fileobject","title":"Uploading a File/Object","text":"<p>Uploading a file to an S3 bucket using Boto3 offers multiple methods, providing flexibility to suit different use cases. One common approach is to use the <code>upload_file</code> method of the S3 client, specifying the local file path and the destination bucket and object key. Alternatively, you can utilize the <code>put_object</code> method, providing the object's content and metadata directly from your Python code. For more extensive or streaming uploads, you can use the multipart upload feature by breaking the file into smaller parts and sending them concurrently, which can be especially useful for large files (I don't have any implementation sample of this at the moment \ud83d\ude05). A sample implementation and usage is shown below:</p> aws_s3/api.pyaws_s3/__init__.pyapp.py <pre><code>from __future__ import annotations\n\nimport functools\nimport os\nfrom typing import Any, Dict, Optional, Tuple, Type, Union\n\nimport boto3 as aws_sdk\nfrom mypy_boto3_s3 import S3Client\nfrom mypy_boto3_s3.client import BotocoreClientError\nfrom mypy_boto3_s3.type_defs import PutObjectOutputTypeDef\n\n\n@functools.lru_cache(maxsize=2**0)\ndef __get_client() -&gt; S3Client:\n    session = aws_sdk.Session()\n    return session.client(\"s3\")\n\n\n@functools.lru_cache(maxsize=2**0)\ndef __get_exceptions() -&gt; Tuple[Type[BotocoreClientError], ...]:\n    client = __get_client()\n    return (\n        client.exceptions.BucketAlreadyExists,\n        client.exceptions.BucketAlreadyOwnedByYou,\n        client.exceptions.ClientError,\n        client.exceptions.InvalidObjectState,\n        client.exceptions.NoSuchBucket,\n        client.exceptions.NoSuchKey,\n        client.exceptions.NoSuchUpload,\n        client.exceptions.ObjectAlreadyInActiveTierError,\n        client.exceptions.ObjectNotInActiveTierError,\n    )\n\n\ndef put_object(\n    bucket: str, body: bytes, key: str, *, metadata: Optional[Dict[str, str]] = None\n) -&gt; Union[PutObjectOutputTypeDef, Dict[str, Any]]:\n    try:\n        client = __get_client()\n        return client.put_object(\n            Bucket=bucket,\n            Body=body,\n            Key=key,\n            ChecksumAlgorithm=\"SHA256\",\n            Metadata=metadata or {},\n        )\n\n    except __get_exceptions() as e:\n        return e.response\n\n\nOBJECT_URI: str = \"s3://{}/{}\"\nPROGRESS_BAR: str = \"{} [{}{}] {} % ({} / {} bytes)...\"\n\n\ndef upload_file(\n    bucket: str, key: str, file_path: str, *, show_progress: bool = False\n) -&gt; Union[Dict[str, Any], None]:\n    if show_progress:\n        uri: str = OBJECT_URI.format(bucket, key)\n        print(\"Uploading {} to {}...\".format(file_path, uri))\n\n    total: int = os.path.getsize(file_path) if show_progress else 0\n    done: int = 0\n\n    def callback(size: int) -&gt; None:\n        nonlocal total\n        nonlocal done\n        if total == 0:\n            return\n        done += size\n        prog: float = done / total  # Range: [0, 1]\n        prog_bar_len: int = 32\n        print(\n            PROGRESS_BAR.format(\n                \"Uploading\",\n                \"=\" * round(prog * prog_bar_len),\n                \" \" * round((1 - prog) * prog_bar_len),\n                round(prog * 100),\n                done,\n                total,\n            ),\n            end=\"\\r\" if prog &lt; 1 else \" Complete.\\n\",\n        )\n\n    try:\n        client = __get_client()\n        return client.upload_file(\n            Bucket=bucket,\n            Key=key,\n            Filename=file_path,\n            Callback=callback if show_progress else None,\n        )\n\n    except __get_exceptions() as e:\n        return e.response\n</code></pre> <pre><code>from .api import put_object, upload_file\n</code></pre> <pre><code>from rich.pretty import pprint\n\nimport aws_s3\n\nbucket: str = \"my-bucket-00000001\"\nfile_path: str = \"sample.json\"\nkey: str = \"uploaded-sample.json\"\n\n# Upload using `put_object`\nwith open(file_path, \"rb\") as buffer:\n    response = aws_s3.put_object(bucket, buffer.read(), key)\n    pprint(response, indent_guides=False)\n\n# Upload using `upload_file`\naws_s3.upload_file(bucket, key, file_path, show_progress=True)\n</code></pre> <p>Output:</p> Using <code>put_object</code>Using <code>upload_file</code> <pre><code>{\n    \"ResponseMetadata\": {\n        \"RequestId\": \"****************\",\n        \"HostId\": \"****************\",\n        \"HTTPStatusCode\": 200,\n        \"HTTPHeaders\": {\n            \"x-amz-id-2\": \"****************\",\n            \"x-amz-request-id\": \"****************\",\n            \"date\": \"Sun, 29 Oct 2023 13:20:01 GMT\",\n            \"x-amz-server-side-encryption\": \"AES256\",\n            \"etag\": '\"5bd9fe559a5630150c700f98bf10fe79\"',\n            \"x-amz-checksum-sha256\": \"ur0o+fCAhHZLxRSy4hx5hxxLVLpMqEdEEp4VD+qyCc4=\",\n            \"server\": \"AmazonS3\",\n            \"content-length\": \"0\",\n            \"connection\": \"close\",\n        },\n        \"RetryAttempts\": 0,\n    },\n    \"ETag\": '\"5bd9fe559a5630150c700f98bf10fe79\"',\n    \"ChecksumSHA256\": \"ur0o+fCAhHZLxRSy4hx5hxxLVLpMqEdEEp4VD+qyCc4=\",\n    \"ServerSideEncryption\": \"AES256\",\n}\n</code></pre> <pre><code>Uploading sample.json to s3://my-bucket-00000001/uploaded-sample.json...\nUploading [================================] 100 % (28 / 28 bytes)... Complete.\n</code></pre>","tags":["AWS","Python","Software Development"]},{"location":"blog/524ea484-523b-4e98-ab37-bbbdb48af016/#downloading-a-fileobject","title":"Downloading a File/Object","text":"<p>Downloading a file from an Amazon S3 bucket using Boto3 offers various methods to suit different needs. A common approach is using the <code>download_file</code> method of the S3 client, specifying the source bucket and object key, along with the local file path where the object will be saved. Alternatively, you can retrieve the object's contents as bytes using the <code>get_object</code> method and then write it to a local file. For more extensive or streaming downloads, you can leverage the S3 Transfer Manager to handle large objects efficiently (Again, I don't have any implementation sample of this at the moment \ud83d\ude05). These methods provide flexibility for fetching objects from your S3 bucket, whether it's a simple download or a more complex operation, all while benefiting from Boto3's ease of use and extensive AWS integration. A sample implementation and usage is shown below:</p> aws_s3/api.pyaws_s3/__init__.pyapp.py <pre><code>from __future__ import annotations\n\nimport functools\nfrom typing import Any, Dict, Tuple, Type, Union\n\nimport boto3 as aws_sdk\nfrom mypy_boto3_s3 import S3Client\nfrom mypy_boto3_s3.client import BotocoreClientError\nfrom mypy_boto3_s3.type_defs import GetObjectOutputTypeDef, HeadObjectOutputTypeDef\n\n\n@functools.lru_cache(maxsize=2**0)\ndef __get_client() -&gt; S3Client:\n    session = aws_sdk.Session()\n    return session.client(\"s3\")\n\n\n@functools.lru_cache(maxsize=2**0)\ndef __get_exceptions() -&gt; Tuple[Type[BotocoreClientError], ...]:\n    client = __get_client()\n    return (\n        client.exceptions.BucketAlreadyExists,\n        client.exceptions.BucketAlreadyOwnedByYou,\n        client.exceptions.ClientError,\n        client.exceptions.InvalidObjectState,\n        client.exceptions.NoSuchBucket,\n        client.exceptions.NoSuchKey,\n        client.exceptions.NoSuchUpload,\n        client.exceptions.ObjectAlreadyInActiveTierError,\n        client.exceptions.ObjectNotInActiveTierError,\n    )\n\n\ndef get_object(bucket: str, key: str) -&gt; Union[GetObjectOutputTypeDef, Dict[str, Any]]:\n    try:\n        client = __get_client()\n        return client.get_object(Bucket=bucket, Key=key)\n\n    except __get_exceptions() as e:\n        return e.response\n\n\n@functools.lru_cache(maxsize=2**16)\ndef get_object_metadata(\n    bucket: str, key: str\n) -&gt; Union[HeadObjectOutputTypeDef, Dict[str, Any]]:\n    try:\n        client = __get_client()\n        return client.head_object(Bucket=bucket, Key=key)\n\n    except __get_exceptions() as e:\n        return e.response\n\n\nOBJECT_URI: str = \"s3://{}/{}\"\nPROGRESS_BAR: str = \"{} [{}{}] {} % ({} / {} bytes)...\"\n\n\ndef download_file(\n    bucket: str, key: str, file_path: str, *, show_progress: bool = False\n) -&gt; Union[Dict[str, Any], None]:\n    if show_progress:\n        uri: str = OBJECT_URI.format(bucket, key)\n        print(\"Downloading {} to {}...\".format(uri, file_path))\n\n    headers = get_object_metadata(bucket, key)\n    total: int = headers[\"ContentLength\"] if show_progress else 0\n    done: int = 0\n\n    def callback(size: int) -&gt; None:\n        nonlocal total\n        nonlocal done\n        if total == 0:\n            return\n        done += size\n        progress: float = done / total  # Range: [0, 1]\n        progress_bar_length: int = 32\n        print(\n            PROGRESS_BAR.format(\n                \"Downloading\",\n                \"=\" * round(progress * progress_bar_length),\n                \" \" * round((1 - progress) * progress_bar_length),\n                round(progress * 100, 1),\n                done,\n                total,\n            ),\n            end=\"\\r\" if progress &lt; 1 else \" Complete.\\n\",\n        )\n\n    try:\n        client = __get_client()\n        return client.download_file(\n            Bucket=bucket,\n            Key=key,\n            Filename=file_path,\n            Callback=callback if show_progress else None,\n        )\n\n    except __get_exceptions() as e:\n        return e.response\n</code></pre> <pre><code>from .api import get_object, download_file\n</code></pre> <pre><code>from rich.pretty import pprint\n\nimport aws_s3\n\nbucket: str = \"my-bucket-00000001\"\nfile_path: str = \"sample.json\"\nkey: str = \"uploaded-sample.json\"\n\n# Download using `get_object`\nwith open(file_path, \"wb\") as buffer:\n    response = aws_s3.get_object(bucket, key)\n    pprint(response, indent_guides=False)\n    body = response[\"Body\"]\n    buffer.write(body.read())\n\n# Download using `download_file`\naws_s3.download_file(bucket, key, file_path, show_progress=True)\n</code></pre> <p>Output:</p> Using <code>get_object</code>Using <code>download_file</code> <pre><code>{\n    'ResponseMetadata': {\n        'RequestId': '****************',\n        'HostId': '****************',\n        'HTTPStatusCode': 200,\n        'HTTPHeaders': {\n            'x-amz-id-2': '****************',\n            'x-amz-request-id': '****************',\n            'date': 'Sun, 29 Oct 2023 13:49:34 GMT',\n            'last-modified': 'Sun, 29 Oct 2023 13:47:57 GMT',\n            'etag': '\"5bd9fe559a5630150c700f98bf10fe79\"',\n            'x-amz-server-side-encryption': 'AES256',\n            'accept-ranges': 'bytes',\n            'content-type': 'binary/octet-stream',\n            'server': 'AmazonS3',\n            'content-length': '28'\n        },\n        'RetryAttempts': 0\n    },\n    'AcceptRanges': 'bytes',\n    'LastModified': datetime.datetime(2023, 10, 29, 13, 47, 57, tzinfo=tzutc()),\n    'ContentLength': 28,\n    'ETag': '\"5bd9fe559a5630150c700f98bf10fe79\"',\n    'ContentType': 'binary/octet-stream',\n    'ServerSideEncryption': 'AES256',\n    'Metadata': {},\n    'Body': &lt;botocore.response.StreamingBody object at 0x7fa9c2028340&gt;\n}\n</code></pre> <pre><code>Downloading s3://my-bucket-00000001/uploaded-sample.json to sample.json...\nDownloading [================================] 100.0 % (28 / 28 bytes)... Complete.\n</code></pre>","tags":["AWS","Python","Software Development"]},{"location":"blog/524ea484-523b-4e98-ab37-bbbdb48af016/#copying-a-fileobject","title":"Copying a File/Object","text":"<p>To duplicate objects within the same bucket or across different S3 buckets, use the <code>copy_object</code> method provided by the S3 client, specifying the source and destination bucket names and object keys. This operation duplicates the object, preserving its metadata and content, and can be useful for tasks like creating backups, moving objects between buckets, or creating replicas. Boto3 simplifies the object copying process, making it a powerful tool for managing data in your S3 storage efficiently. A sample implementation and usage is shown below:</p> aws_s3/api.pyaws_s3/__init__.pyapp.py <pre><code>from __future__ import annotations\n\nimport functools\nfrom typing import Any, Dict, Optional, Tuple, Type, Union\n\nimport boto3 as aws_sdk\nfrom mypy_boto3_s3 import S3Client\nfrom mypy_boto3_s3.client import BotocoreClientError\nfrom mypy_boto3_s3.type_defs import CopyObjectOutputTypeDef, CopySourceTypeDef\n\n\n@functools.lru_cache(maxsize=2**0)\ndef __get_client() -&gt; S3Client:\n    session = aws_sdk.Session()\n    return session.client(\"s3\")\n\n\n@functools.lru_cache(maxsize=2**0)\ndef __get_exceptions() -&gt; Tuple[Type[BotocoreClientError], ...]:\n    client = __get_client()\n    return (\n        client.exceptions.BucketAlreadyExists,\n        client.exceptions.BucketAlreadyOwnedByYou,\n        client.exceptions.ClientError,\n        client.exceptions.InvalidObjectState,\n        client.exceptions.NoSuchBucket,\n        client.exceptions.NoSuchKey,\n        client.exceptions.NoSuchUpload,\n        client.exceptions.ObjectAlreadyInActiveTierError,\n        client.exceptions.ObjectNotInActiveTierError,\n    )\n\n\ndef copy_object(\n    source_bucket: str,\n    source_key: str,\n    target_bucket: str,\n    target_key: str,\n    *,\n    metadata: Optional[Dict[str, str]] = None,\n) -&gt; Union[CopyObjectOutputTypeDef, Dict[str, Any]]:\n    try:\n        client = __get_client()\n        return client.copy_object(\n            Bucket=target_bucket,\n            Key=target_key,\n            CopySource=CopySourceTypeDef(Bucket=source_bucket, Key=source_key),\n            Metadata=metadata or {},\n        )\n\n    except __get_exceptions() as e:\n        return e.response\n</code></pre> <pre><code>from .api import copy_object\n</code></pre> <pre><code>from rich.pretty import pprint\n\nimport aws_s3\n\nsource_bucket: str = \"my-bucket-00000001\"\nsource_key: str = \"upload-sample.json\"\ntarget_bucket: str = \"my-bucket-00000001\"\ntarget_key: str = \"upload-sample-copy.json\"\n\nresult = aws_s3.copy_object(source_bucket, source_key, target_bucket, target_key)\npprint(result, indent_guides=False)\n</code></pre> <p>Output:</p> <pre><code>{\n    \"ResponseMetadata\": {\n        \"RequestId\": \"****************\",\n        \"HostId\": \"****************\",\n        \"HTTPStatusCode\": 200,\n        \"HTTPHeaders\": {\n            \"x-amz-id-2\": \"****************\",\n            \"x-amz-request-id\": \"****************\",\n            \"date\": \"Sun, 29 Oct 2023 15:16:48 GMT\",\n            \"x-amz-server-side-encryption\": \"AES256\",\n            \"content-type\": \"application/xml\",\n            \"server\": \"AmazonS3\",\n            \"content-length\": \"224\",\n        },\n        \"RetryAttempts\": 0,\n    },\n    \"ServerSideEncryption\": \"AES256\",\n    \"CopyObjectResult\": {\n        \"ETag\": '\"5bd9fe559a5630150c700f98bf10fe79\"',\n        \"LastModified\": datetime.datetime(2023, 10, 29, 15, 16, 48, tzinfo=tzutc()),\n    },\n}\n</code></pre>","tags":["AWS","Python","Software Development"]},{"location":"blog/524ea484-523b-4e98-ab37-bbbdb48af016/#deleting-filesobjects","title":"Deleting Files/Objects","text":"<p>To delete a single object, you can use the <code>delete_object</code> method provided by the S3 client, specifying the bucket name and the object key. This deletes the specified object from the S3 bucket. For deleting multiple objects, you can use the <code>delete_objects</code> method, passing a list of object keys to be removed in a single request. Handling deletions with Boto3 simplifies the task of managing your S3 data, whether you need to remove individual objects or perform bulk deletions, ensuring that your storage resources remain organized and uncluttered. A sample implementation and usage is shown below:</p> aws_s3/api.pyaws_s3/__init__.pyapp.py <pre><code>from __future__ import annotations\n\nimport functools\nfrom typing import Any, Dict, List, Tuple, Type, Union\n\nimport boto3 as aws_sdk\nfrom mypy_boto3_s3 import S3Client\nfrom mypy_boto3_s3.client import BotocoreClientError\nfrom mypy_boto3_s3.type_defs import (\n    DeleteObjectOutputTypeDef,\n    DeleteObjectsOutputTypeDef,\n    DeleteTypeDef,\n    ObjectIdentifierTypeDef,\n)\n\n\n@functools.lru_cache(maxsize=2**0)\ndef __get_client() -&gt; S3Client:\n    session = aws_sdk.Session()\n    return session.client(\"s3\")\n\n\n@functools.lru_cache(maxsize=2**0)\ndef __get_exceptions() -&gt; Tuple[Type[BotocoreClientError], ...]:\n    client = __get_client()\n    return (\n        client.exceptions.BucketAlreadyExists,\n        client.exceptions.BucketAlreadyOwnedByYou,\n        client.exceptions.ClientError,\n        client.exceptions.InvalidObjectState,\n        client.exceptions.NoSuchBucket,\n        client.exceptions.NoSuchKey,\n        client.exceptions.NoSuchUpload,\n        client.exceptions.ObjectAlreadyInActiveTierError,\n        client.exceptions.ObjectNotInActiveTierError,\n    )\n\n\ndef delete_object(\n    bucket: str, key: str\n) -&gt; Union[DeleteObjectOutputTypeDef, Dict[str, Any]]:\n    try:\n        client = __get_client()\n        return client.delete_object(Bucket=bucket, Key=key)\n\n    except __get_exceptions() as e:\n        return e.response\n\n\ndef delete_objects(\n    bucket: str, keys: List[str]\n) -&gt; Union[DeleteObjectsOutputTypeDef, Dict[str, Any]]:\n    try:\n        client = __get_client()\n        return client.delete_objects(\n            Bucket=bucket,\n            Delete=DeleteTypeDef(\n                Objects=[ObjectIdentifierTypeDef(Key=each_key) for each_key in keys]\n            ),\n        )\n\n    except __get_exceptions() as e:\n        return e.response\n</code></pre> <pre><code>from .api import delete_object, delete_objects\n</code></pre> <pre><code>from rich.pretty import pprint\n\nimport aws_s3\n\nbucket: str = \"my-bucket-00000001\"\nkey: str = \"upload-sample.json\"\nkey_copy: str = \"upload-sample-copy.json\"\n\nresult = aws_s3.delete_object(bucket, key)\npprint(result, indent_guides=False)\n\nresult = aws_s3.delete_objects(bucket, [key, key_copy])\npprint(result, indent_guides=False)\n</code></pre> <p>Output:</p> Using <code>delete_object</code>Using <code>delete_objects</code> <pre><code>{\n    \"ResponseMetadata\": {\n        \"RequestId\": \"****************\",\n        \"HostId\": \"****************\",\n        \"HTTPStatusCode\": 204,\n        \"HTTPHeaders\": {\n            \"x-amz-id-2\": \"****************\",\n            \"x-amz-request-id\": \"****************\",\n            \"date\": \"Sun, 29 Oct 2023 15:27:08 GMT\",\n            \"server\": \"AmazonS3\",\n        },\n        \"RetryAttempts\": 0,\n    }\n}\n</code></pre> <pre><code>{\n    \"ResponseMetadata\": {\n        \"RequestId\": \"****************\",\n        \"HostId\": \"****************\",\n        \"HTTPStatusCode\": 200,\n        \"HTTPHeaders\": {\n            \"x-amz-id-2\": \"****************\",\n            \"x-amz-request-id\": \"****************\",\n            \"date\": \"Sun, 29 Oct 2023 15:27:08 GMT\",\n            \"content-type\": \"application/xml\",\n            \"transfer-encoding\": \"chunked\",\n            \"server\": \"AmazonS3\",\n            \"connection\": \"close\",\n        },\n        \"RetryAttempts\": 0,\n    },\n    \"Deleted\": [{\"Key\": \"upload-sample.json\"}, {\"Key\": \"upload-sample-copy.json\"}],\n}\n</code></pre>","tags":["AWS","Python","Software Development"]},{"location":"blog/524ea484-523b-4e98-ab37-bbbdb48af016/#moving-a-fileobject","title":"Moving a File/Object","text":"<p>As of writing, the S3 client does not have a method specifically for moving an S3 object within a bucket or to other buckets. However, we can implement it by combining the <code>copy_object</code> and <code>delete_object</code>, in that order. A sample implementation and usage is shown below:</p> aws_s3/api.pyaws_s3/__init__.pyapp.py <pre><code>from __future__ import annotations\n\nimport functools\nfrom typing import Any, Dict, Optional, Tuple, Type, Union\n\nimport boto3 as aws_sdk\nfrom mypy_boto3_s3 import S3Client\nfrom mypy_boto3_s3.client import BotocoreClientError\nfrom mypy_boto3_s3.type_defs import (\n    CopyObjectOutputTypeDef,\n    CopySourceTypeDef,\n    DeleteObjectOutputTypeDef,\n)\n\n\n@functools.lru_cache(maxsize=2**0)\ndef __get_client() -&gt; S3Client:\n    session = aws_sdk.Session()\n    return session.client(\"s3\")\n\n\n@functools.lru_cache(maxsize=2**0)\ndef __get_exceptions() -&gt; Tuple[Type[BotocoreClientError], ...]:\n    client = __get_client()\n    return (\n        client.exceptions.BucketAlreadyExists,\n        client.exceptions.BucketAlreadyOwnedByYou,\n        client.exceptions.ClientError,\n        client.exceptions.InvalidObjectState,\n        client.exceptions.NoSuchBucket,\n        client.exceptions.NoSuchKey,\n        client.exceptions.NoSuchUpload,\n        client.exceptions.ObjectAlreadyInActiveTierError,\n        client.exceptions.ObjectNotInActiveTierError,\n    )\n\n\ndef copy_object(\n    source_bucket: str,\n    source_key: str,\n    target_bucket: str,\n    target_key: str,\n    *,\n    metadata: Optional[Dict[str, str]] = None,\n) -&gt; Union[CopyObjectOutputTypeDef, Dict[str, Any]]:\n    try:\n        client = __get_client()\n        return client.copy_object(\n            Bucket=target_bucket,\n            Key=target_key,\n            CopySource=CopySourceTypeDef(Bucket=source_bucket, Key=source_key),\n            Metadata=metadata or {},\n        )\n\n    except __get_exceptions() as e:\n        return e.response\n\n\ndef delete_object(\n    bucket: str, key: str\n) -&gt; Union[DeleteObjectOutputTypeDef, Dict[str, Any]]:\n    try:\n        client = __get_client()\n        return client.delete_object(Bucket=bucket, Key=key)\n\n    except __get_exceptions() as e:\n        return e.response\n\n\ndef move_object(\n    source_bucket: str,\n    source_key: str,\n    target_bucket: str,\n    target_key: str,\n    *,\n    metadata: Optional[Dict[str, str]] = None,\n) -&gt; Union[Dict[str, Any], None]:\n    try:\n        copy_object(\n            source_bucket, source_key, target_bucket, target_key, metadata=metadata\n        )\n        delete_object(source_bucket, source_key)\n\n    except __get_exceptions() as e:\n        return e.response\n</code></pre> <pre><code>from .api import move_object\n</code></pre> <pre><code>\n</code></pre>","tags":["AWS","Python","Software Development"]},{"location":"blog/524ea484-523b-4e98-ab37-bbbdb48af016/#final-code","title":"Final Code","text":"<p>Below is the full implementation of all the S3 bucket and object client methods discussed above:</p> requirements.txtaws_s3/__init__.pyaws_s3/api.py <pre><code>boto3==1.28.63\nmypy-boto3-s3==1.28.55\nrich\n</code></pre> <pre><code>from .api import (\n    copy_object,\n    create_bucket,\n    delete_bucket,\n    delete_object,\n    delete_objects,\n    download_file,\n    generate_presigned_url,\n    get_bucket_metadata,\n    get_object,\n    get_object_metadata,\n    list_buckets,\n    list_objects,\n    move_object,\n    put_object,\n    upload_file,\n)\n</code></pre> <pre><code>from __future__ import annotations\n\nimport functools\nimport os\nfrom typing import Any, Dict, List, Literal, Optional, Tuple, Type, Union, cast\n\nimport boto3 as aws_sdk\nfrom mypy_boto3_s3 import S3Client\nfrom mypy_boto3_s3.client import BotocoreClientError\nfrom mypy_boto3_s3.paginator import ListObjectsV2Paginator\nfrom mypy_boto3_s3.type_defs import (\n    CopyObjectOutputTypeDef,\n    CopySourceTypeDef,\n    CreateBucketConfigurationTypeDef,\n    CreateBucketOutputTypeDef,\n    DeleteObjectOutputTypeDef,\n    DeleteObjectsOutputTypeDef,\n    DeleteTypeDef,\n    EmptyResponseMetadataTypeDef,\n    GetObjectOutputTypeDef,\n    HeadObjectOutputTypeDef,\n    ListBucketsOutputTypeDef,\n    ObjectIdentifierTypeDef,\n    ObjectTypeDef,\n    PutObjectOutputTypeDef,\n)\n\n\n@functools.lru_cache(maxsize=2**0)\ndef __get_client() -&gt; S3Client:\n    session = aws_sdk.Session()\n    return session.client(\"s3\")\n\n\n@functools.lru_cache(maxsize=2**0)\ndef __get_exceptions() -&gt; Tuple[Type[BotocoreClientError], ...]:\n    client = __get_client()\n    return (\n        client.exceptions.BucketAlreadyExists,\n        client.exceptions.BucketAlreadyOwnedByYou,\n        client.exceptions.ClientError,\n        client.exceptions.InvalidObjectState,\n        client.exceptions.NoSuchBucket,\n        client.exceptions.NoSuchKey,\n        client.exceptions.NoSuchUpload,\n        client.exceptions.ObjectAlreadyInActiveTierError,\n        client.exceptions.ObjectNotInActiveTierError,\n    )\n\n\ndef create_bucket(\n    bucket: str, region: Optional[str] = None\n) -&gt; Union[CreateBucketOutputTypeDef, Dict[str, Any]]:\n    \"\"\"\n    Create an AWS S3 bucket with name `bucket` in region `region`. If `region` is not\n    defined, then `region` will be set to `us-east-1`.\n    \"\"\"\n    try:\n        client = __get_client()\n        return client.create_bucket(\n            Bucket=bucket,\n            CreateBucketConfiguration=CreateBucketConfigurationTypeDef(\n                LocationConstraint=region or \"us-east-1\"\n            ),\n        )\n\n    except __get_exceptions() as e:\n        return e.response\n\n\ndef get_bucket_metadata(\n    bucket: str,\n) -&gt; Union[EmptyResponseMetadataTypeDef, Dict[str, Any]]:\n    try:\n        client = __get_client()\n        return client.head_bucket(Bucket=bucket)\n\n    except __get_exceptions() as e:\n        return e.response\n\n\ndef list_buckets() -&gt; Union[ListBucketsOutputTypeDef, Dict[str, Any]]:\n    try:\n        client = __get_client()\n        return client.list_buckets()\n\n    except __get_exceptions() as e:\n        return e.response\n\n\ndef delete_bucket(bucket: str) -&gt; Dict[str, Any]:\n    try:\n        client = __get_client()\n        return client.delete_bucket(Bucket=bucket)\n\n    except __get_exceptions() as e:\n        return e.response\n\n\nHTTPVerb = Literal[\"GET\", \"PUT\", \"DELETE\", \"HEAD\"]\n\n\ndef generate_presigned_url(\n    method: HTTPVerb, bucket: str, key: str, *, ttl: int = 3600\n) -&gt; Union[str, Dict[str, Any]]:\n    \"\"\"\n    Generate a URL for uploading (`PUT`), downloading (`GET`), or deleting (`DELETE`)\n    objects to/from/in an S3 bucket. If `ttl` is not defined, then the generated URL\n    will be valid for 1 hour (3600 seconds).\n\n    Usage:\n\n    &gt;&gt;&gt; import aws_s3\n    &gt;&gt;&gt; import requests\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; bucket: str = \"sample-bucket\"\n    &gt;&gt;&gt; file_path: str = \"sample.json\"\n    &gt;&gt;&gt; key: str = \"upload/sample.json\"\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Uploading an object:\n    &gt;&gt;&gt; with open(file_path, \"rb\") as buffer:\n    &gt;&gt;&gt;     url = aws_s3.generate_presigned_url(\"PUT\", bucket, key)\n    &gt;&gt;&gt;     print(f\"Generated URL: {url}\")\n    &gt;&gt;&gt;     response = requests.put(url, data=buffer.read())\n    &gt;&gt;&gt;     assert response.status_code == 200, \"HTTP Status Code 200\"\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Downloading an object:\n    &gt;&gt;&gt; with open(file_path, \"wb\") as buffer:\n    &gt;&gt;&gt;     url = aws_s3.generate_presigned_url(\"GET\", bucket, key)\n    &gt;&gt;&gt;     print(f\"Generated URL: {url}\")\n    &gt;&gt;&gt;     response = requests.get(url)\n    &gt;&gt;&gt;     buffer.write(response.content)\n    &gt;&gt;&gt;     assert response.status_code == 200, \"HTTP Status Code 200\"\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Deleting an object:\n    &gt;&gt;&gt; url = aws_s3.generate_presigned_url(\"DELETE\", bucket, key)\n    &gt;&gt;&gt; print(f\"Generated URL: {url}\")\n    &gt;&gt;&gt; response = requests.delete(url)\n    &gt;&gt;&gt; assert response.status_code == 204, \"HTTP Status Code 204\"\n\n    Reference/s:\n        - [StackOverflow](https://stackoverflow.com/a/69376282)\n        - [AWS S3 Docs](https://tinyurl.com/2p87uuum)\n    \"\"\"\n    try:\n        client = __get_client()\n        CLIENT_METHOD_DISPATCH: Dict[HTTPVerb, str] = {\n            \"GET\": client.get_object.__name__,\n            \"PUT\": client.put_object.__name__,\n            \"DELETE\": client.delete_object.__name__,\n            \"HEAD\": client.head_object.__name__,\n        }\n        return client.generate_presigned_url(\n            ClientMethod=CLIENT_METHOD_DISPATCH[method],\n            Params={\"Bucket\": bucket, \"Key\": key},\n            ExpiresIn=ttl,\n        )\n\n    except __get_exceptions() as e:\n        return e.response\n\n\ndef put_object(\n    bucket: str, body: bytes, key: str, *, metadata: Optional[Dict[str, str]] = None\n) -&gt; Union[PutObjectOutputTypeDef, Dict[str, Any]]:\n    try:\n        client = __get_client()\n        return client.put_object(\n            Bucket=bucket,\n            Body=body,\n            Key=key,\n            ChecksumAlgorithm=\"SHA256\",\n            Metadata=metadata or {},\n        )\n\n    except __get_exceptions() as e:\n        return e.response\n\n\ndef get_object(bucket: str, key: str) -&gt; Union[GetObjectOutputTypeDef, Dict[str, Any]]:\n    try:\n        client = __get_client()\n        return client.get_object(Bucket=bucket, Key=key)\n\n    except __get_exceptions() as e:\n        return e.response\n\n\n@functools.lru_cache(maxsize=2**16)\ndef get_object_metadata(\n    bucket: str, key: str\n) -&gt; Union[HeadObjectOutputTypeDef, Dict[str, Any]]:\n    try:\n        client = __get_client()\n        return client.head_object(Bucket=bucket, Key=key)\n\n    except __get_exceptions() as e:\n        return e.response\n\n\ndef list_objects(\n    bucket: str, prefix: Optional[str] = None\n) -&gt; Union[List[ObjectTypeDef], Dict[str, Any]]:\n    \"\"\"\n    Reference/s:\n        - [StackOverflow](https://stackoverflow.com/a/59816089)\n    \"\"\"\n    try:\n        client = __get_client()\n        paginator = cast(\n            ListObjectsV2Paginator,\n            client.get_paginator(client.list_objects_v2.__name__),\n        )\n\n        result: List[ObjectTypeDef] = []\n        for each_page in paginator.paginate(Bucket=bucket, Prefix=prefix or \"\"):\n            result.extend(each_page[\"Contents\"])\n\n        return result\n\n    except __get_exceptions() as e:\n        return e.response\n\n\nOBJECT_URI: str = \"s3://{}/{}\"\nPROGRESS_BAR: str = \"{} [{}{}] {} % ({} / {} bytes)...\"\n\n\ndef upload_file(\n    bucket: str, key: str, file_path: str, *, show_progress: bool = False\n) -&gt; Union[Dict[str, Any], None]:\n    \"\"\"\n    Upload a file from local device to an S3 bucket.\n    \"\"\"\n    if show_progress:\n        uri: str = OBJECT_URI.format(bucket, key)\n        print(\"Uploading {} to {}...\".format(file_path, uri))\n\n    total: int = os.path.getsize(file_path) if show_progress else 0\n    done: int = 0\n\n    def callback(size: int) -&gt; None:\n        nonlocal total\n        nonlocal done\n        if total == 0:\n            return\n        done += size\n        prog: float = done / total  # Range: [0, 1]\n        prog_bar_len: int = 32\n        print(\n            PROGRESS_BAR.format(\n                \"Uploading\",\n                \"=\" * round(prog * prog_bar_len),\n                \" \" * round((1 - prog) * prog_bar_len),\n                round(prog * 100),\n                done,\n                total,\n            ),\n            end=\"\\r\" if prog &lt; 1 else \" Complete.\\n\",\n        )\n\n    try:\n        client = __get_client()\n        return client.upload_file(\n            Bucket=bucket,\n            Key=key,\n            Filename=file_path,\n            Callback=callback if show_progress else None,\n        )\n\n    except __get_exceptions() as e:\n        return e.response\n\n\ndef download_file(\n    bucket: str, key: str, file_path: str, *, show_progress: bool = False\n) -&gt; Union[Dict[str, Any], None]:\n    \"\"\"\n    Download a file from a bucket `bucket` with key `key` to local device\n    with file path `file_path`. If `show_progress` is `True`, then a\n    progress bar will be displayed in the console.\n    \"\"\"\n    if show_progress:\n        uri: str = OBJECT_URI.format(bucket, key)\n        print(\"Downloading {} to {}...\".format(uri, file_path))\n\n    headers = get_object_metadata(bucket, key)\n    total: int = headers[\"ContentLength\"] if show_progress else 0\n    done: int = 0\n\n    def callback(size: int) -&gt; None:\n        nonlocal total\n        nonlocal done\n        if total == 0:\n            return\n        done += size\n        progress: float = done / total  # Range: [0, 1]\n        progress_bar_length: int = 32\n        print(\n            PROGRESS_BAR.format(\n                \"Downloading\",\n                \"=\" * round(progress * progress_bar_length),\n                \" \" * round((1 - progress) * progress_bar_length),\n                round(progress * 100, 1),\n                done,\n                total,\n            ),\n            end=\"\\r\" if progress &lt; 1 else \" Complete.\\n\",\n        )\n\n    try:\n        client = __get_client()\n        return client.download_file(\n            Bucket=bucket,\n            Key=key,\n            Filename=file_path,\n            Callback=callback if show_progress else None,\n        )\n\n    except __get_exceptions() as e:\n        return e.response\n\n\ndef copy_object(\n    source_bucket: str,\n    source_key: str,\n    target_bucket: str,\n    target_key: str,\n    *,\n    metadata: Optional[Dict[str, str]] = None,\n) -&gt; Union[CopyObjectOutputTypeDef, Dict[str, Any]]:\n    \"\"\"\n    Copy an object with key `source_key` from bucket `source_bucket` to\n    another bucket `target_bucket` with key `target_key`.\n    \"\"\"\n    try:\n        client = __get_client()\n        return client.copy_object(\n            Bucket=target_bucket,\n            Key=target_key,\n            CopySource=CopySourceTypeDef(Bucket=source_bucket, Key=source_key),\n            Metadata=metadata or {},\n        )\n\n    except __get_exceptions() as e:\n        return e.response\n\n\ndef delete_object(\n    bucket: str, key: str\n) -&gt; Union[DeleteObjectOutputTypeDef, Dict[str, Any]]:\n    \"\"\"\n    Delete an object from bucket `bucket` with key `key`.\n    \"\"\"\n    try:\n        client = __get_client()\n        return client.delete_object(Bucket=bucket, Key=key)\n\n    except __get_exceptions() as e:\n        return e.response\n\n\ndef delete_objects(\n    bucket: str, keys: List[str]\n) -&gt; Union[DeleteObjectsOutputTypeDef, Dict[str, Any]]:\n    \"\"\"\n    Delete object/s from bucket `bucket` if key is in list `keys`.\n    \"\"\"\n    try:\n        client = __get_client()\n        return client.delete_objects(\n            Bucket=bucket,\n            Delete=DeleteTypeDef(\n                Objects=[ObjectIdentifierTypeDef(Key=each_key) for each_key in keys]\n            ),\n        )\n\n    except __get_exceptions() as e:\n        return e.response\n\n\ndef move_object(\n    source_bucket: str,\n    source_key: str,\n    target_bucket: str,\n    target_key: str,\n    *,\n    metadata: Optional[Dict[str, str]] = None,\n) -&gt; Union[Dict[str, Any], None]:\n    try:\n        copy_object(\n            source_bucket, source_key, target_bucket, target_key, metadata=metadata\n        )\n        delete_object(source_bucket, source_key)\n\n    except __get_exceptions() as e:\n        return e.response\n</code></pre>","tags":["AWS","Python","Software Development"]},{"location":"blog/524ea484-523b-4e98-ab37-bbbdb48af016/#use-cases-and-benefits","title":"Use Cases and Benefits","text":"<p>Managing S3 buckets and objects effectively can provide various benefits for individuals and organizations. Here is a list of possible benefits:</p> <ol> <li>Scalable Storage: S3 allows you to store an unlimited amount of data, making it a highly scalable solution for your storage needs.</li> <li>Durability: S3 provides 99.999999999% (11 nines) data durability, ensuring that your data is highly resistant to loss.</li> <li>Availability: Objects stored in S3 are highly available, with an SLA (Service Level Agreement) of 99.99% uptime.</li> <li>Cost-Effective: S3 offers a cost-effective storage solution, and you only pay for the storage you use.</li> <li>Data Security: S3 provides multiple layers of data security, including encryption at rest and in transit, access control policies, and versioning.</li> <li>Data Backup: S3 is an excellent platform for data backup, providing a secure and scalable solution for disaster recovery.</li> <li>Data Archiving: You can use S3's storage classes, such as Glacier, for long-term data archiving, reducing costs further.</li> <li>Data Encryption: S3 supports server-side and client-side encryption, ensuring data security and privacy.</li> </ol>","tags":["AWS","Python","Software Development"]},{"location":"blog/524ea484-523b-4e98-ab37-bbbdb48af016/#conclusion","title":"Conclusion","text":"<p>In conclusion, Boto3 provides a powerful and user-friendly toolkit for managing Amazon S3 buckets and objects, making it an essential tool for AWS developers and administrators. With Boto3, you can easily create and delete buckets, upload and download files, generate presigned URLs, and handle exceptions gracefully. The library offers an array of features to manage and interact with your S3 resources efficiently, making it a valuable asset for everything from data storage and backup to content distribution and data analytics. By following the steps outlined in this article, you can harness the full potential of Boto3 and unlock the capabilities of Amazon S3, ensuring that your cloud-based data management is both robust and accessible.</p>","tags":["AWS","Python","Software Development"]},{"location":"blog/10f91e6d-7407-447c-8b23-8329762b4d8e/","title":"Python Logging Part 1: Handlers and Formatters","text":"<p>Python's standard logging module offers a robust solution for developers to track and manage an application's behavior. This post delves into the practical usage of Python's logging module and sheds light on the role of Handlers and Formatters. Understanding these elements equips developers with the tools needed to enhance code reliability and streamline troubleshooting processes.</p> Changelog <ul> <li>2023-10-06<ul> <li>minor formatting</li> </ul> </li> <li>2023-08-25<ul> <li>moved changelog further up</li> <li>added an example for <code>FileHandler</code></li> <li>renamed some headers</li> </ul> </li> <li>2023-08-24<ul> <li>initially published</li> </ul> </li> </ul>","tags":["Python","Software Development"]},{"location":"blog/10f91e6d-7407-447c-8b23-8329762b4d8e/#handlers","title":"Handlers","text":"<p>In Python's logging module, handlers are components responsible for determining what should be done with log messages once they have been emitted by loggers. Log messages are generated using loggers, and handlers define where these messages are sent, such as to the console, files, remote servers, etc.</p> <p>Handlers in the logging module provide an interface for sending log messages to various destinations. Each handler has a specific purpose and configuration options to control the formatting and filtering of log messages. Some common types of handlers include:</p> <ul> <li><code>StreamHandler</code> : This handler sends log messages to a specified stream, typically the console (<code>sys.stdout</code> or <code>sys.stderr</code>). It's useful for displaying log messages in a terminal or command prompt.</li> <li><code>FileHandler</code> : This handler writes log messages to a specified file. It's often used to create log files for tracking application behavior over time.</li> <li><code>RotatingFileHandler</code> : This handler extends FileHandler and automatically rotates log files based on size or time, creating a new file when the current one reaches a certain size or when a certain time interval has passed.</li> <li><code>TimedRotatingFileHandler</code> : Another extension of FileHandler, this handler rotates log files based on specific time intervals, creating new files at specified intervals (e.g., daily, hourly).</li> <li><code>SocketHandler</code> : This handler sends log messages to a remote server via a network socket. It's useful for logging in distributed systems or remote applications.</li> <li><code>SMTPHandler</code> : This handler sends log messages as emails using the Simple Mail Transfer Protocol (SMTP). It's often used to notify administrators or developers about critical events.</li> <li><code>SysLogHandler</code> : This handler sends log messages to the system log (on Unix-like systems) or the Event Log (on Windows systems).</li> <li><code>NullHandler</code> : This handler is used when you want to disable logging entirely. It's often used as a placeholder when no other suitable handler is configured.</li> </ul>","tags":["Python","Software Development"]},{"location":"blog/10f91e6d-7407-447c-8b23-8329762b4d8e/#formatters","title":"Formatters","text":"<p>In Python's logging module, a formatter is an object used to define the structure and content of log messages. Formatters determine how log records are transformed into human-readable text when they are emitted by loggers and sent to various output destinations like files, console, or remote servers.</p> <p>Formatters allow you to control the layout and appearance of log messages, making them easier to read and understand. You can customize the format using placeholders, which are placeholders enclosed in percentage signs (%). These placeholders are replaced with actual values from the log records when log messages are formatted.</p> <p>Common placeholders include:</p> <ul> <li><code>%asctime%</code> : The timestamp when the log record was created.</li> <li><code>%levelname%</code> : The log level name (e.g., INFO, WARNING, ERROR).</li> <li><code>%name%</code> : The name of the logger that emitted the log record.</li> <li><code>%message%</code> : The actual log message itself.</li> <li><code>%filename%</code> : The name of the source file where the logging call was made.</li> <li><code>%lineno%</code> : The line number in the source file where the logging call was made.</li> <li><code>%funcName%</code> : The name of the function where the logging call was made.</li> </ul>","tags":["Python","Software Development"]},{"location":"blog/10f91e6d-7407-447c-8b23-8329762b4d8e/#usage","title":"Usage","text":"","tags":["Python","Software Development"]},{"location":"blog/10f91e6d-7407-447c-8b23-8329762b4d8e/#streamhandler","title":"<code>StreamHandler</code>","text":"<p>The <code>StreamHandler</code> class is a handler that sends log records to a specified output stream, such as the console (<code>stdout</code>) or a file-like object. The <code>Formatter</code> class is used to define the format of the log messages.</p> <p>Here's an example of how to use <code>StreamHandler</code> and <code>Formatter</code> together:</p> logging_stream_handler.py<pre><code>import logging\nimport time\nimport typing\n\nLOGGER_NAME: typing.Final[str] = \"LoggerName\"\n\nlogger = logging.getLogger(LOGGER_NAME) # (1)!\nlogger.setLevel(logging.DEBUG)\n\nformatter = logging.Formatter( # (2)!\n    (\n        \"%(asctime)s.%(msecs)03d\" + time.strftime(\"%z\") + \" \"\n        \"| %(process)-8d \"\n        \"| %(thread)-16d \"\n        \"| %(levelname)-8s \"\n        \"| %(lineno)-4d \"\n        \"| %(message)s\"\n    ),\n    \"%Y-%m-%dT%H:%M:%S\",\n)\n\nhandler = logging.StreamHandler() # (3)!\nhandler.setLevel(logging.DEBUG)\nhandler.setFormatter(formatter) # (4)!\n\nlogger.addHandler(handler) # (5)!\n\n# Sample\nlogger.debug(\"Debug message.\")\nlogger.info(\"Info message.\")\nlogger.warning(\"Warning message.\")\nlogger.error(\"Error message.\")\nlogger.critical(\"Warning message.\")\nlogger.exception(\"Exception message.\")\n</code></pre> <ol> <li>Return a <code>Logger</code> object named <code>LOGGER_NAME</code>. If such <code>Logger</code> does not exist, then it will be created and returned.</li> <li>Create a <code>Formatter</code> object. This will define how <code>LogRecord</code>s will look like in the output. See the official documentation for all the available <code>LogRecord</code> attributes.</li> <li>Create a <code>StreamHandler</code> object. <code>StreamHandler</code>s send log messages to a specified stream, typically the console (<code>sys.stdout</code> or <code>sys.stderr</code>).</li> <li>Attach the <code>Formatter</code> object to the <code>StreamHandler</code> object.</li> <li>Attach the <code>StreamHandler</code> object to the <code>Logger</code> object.</li> </ol> <p>The above script should run as-is.</p>","tags":["Python","Software Development"]},{"location":"blog/10f91e6d-7407-447c-8b23-8329762b4d8e/#filehandler","title":"<code>FileHandler</code>","text":"<p>In addition to using the <code>StreamHandler</code> to log messages to the console, you can also use the <code>FileHandler</code> to log messages to a file. Here's an example of how to use the <code>FileHandler</code> and <code>Formatter</code> together:</p> logging_file_handler.py<pre><code>import logging\nimport time\nimport typing\n\nLOGGER_NAME: typing.Final[str] = \"LoggerName\"\nLOG_FILE_PATH: typing.Final[str] = \"event.log\"\n\nlogger = logging.getLogger(LOGGER_NAME) # (1)!\nlogger.setLevel(logging.DEBUG)\n\nformatter = logging.Formatter( # (2)!\n    (\n        \"%(asctime)s.%(msecs)03d\" + time.strftime(\"%z\") + \" \"\n        \"| %(process)-8d \"\n        \"| %(thread)-16d \"\n        \"| %(levelname)-8s \"\n        \"| %(lineno)-4d \"\n        \"| %(message)s\"\n    ),\n    \"%Y-%m-%dT%H:%M:%S\",\n)\n\nhandler = logging.FileHandler(LOG_FILE_PATH) # (3)!\nhandler.setLevel(logging.DEBUG)\nhandler.setFormatter(formatter) # (4)!\n\nlogger.addHandler(handler) # (5)!\n\n# NOTE: (6)\nlogger.debug(\"Debug message.\")\nlogger.info(\"Info message.\")\nlogger.warning(\"Warning message.\")\nlogger.error(\"Error message.\")\nlogger.critical(\"Warning message.\")\nlogger.exception(\"Exception message.\")\n</code></pre> <ol> <li>Return a <code>Logger</code> object named <code>LOGGER_NAME</code>. If such <code>Logger</code> does not exist, then it will be created and returned.</li> <li>Create a <code>Formatter</code> object. This will define how <code>LogRecord</code>s will look like in the output. See the official documentation for all the available <code>LogRecord</code> attributes.</li> <li>Create a <code>FileHandler</code> object. <code>FileHandler</code>s writes log messages to a specified file (in this case, in <code>event.log</code>).</li> <li>Attach the <code>Formatter</code> object to the <code>FileHandler</code> object.</li> <li>Attach the <code>FileHandler</code> object to the <code>Logger</code> object.</li> <li>The output logs should be found in a file named <code>event.log</code></li> </ol> <p>The above script should run as-is.</p>","tags":["Python","Software Development"]},{"location":"blog/10f91e6d-7407-447c-8b23-8329762b4d8e/#conclusion","title":"Conclusion","text":"<p>In summary, Python's <code>logging</code> module empowers developers with effective tools to manage and record information within their applications. We've covered key aspects, including loggers, formatters, and practical examples of <code>StreamHandler</code> and <code>FileHandler</code> usage. Getting familiar with this tool enhances your ability to create reliable and maintainable Python projects. Logging aids in understanding application behavior, simplifies debugging, and contributes to robust software development.</p>","tags":["Python","Software Development"]},{"location":"blog/bbcb98c8-dc68-4cb4-a660-8cf4d7c224ae/","title":"Python Logging Part 2: AWS CloudWatch Logs Handler","text":"<p>Incorporating AWS CloudWatch Logs into Python applications via a custom logging handler is a practical approach for robust log management and monitoring. This article will walk you through the straightforward process of setting up and integrating CloudWatch Logs, providing a clear path for developers to enhance their application's log tracking and analysis capabilities.</p> Changelog <ul> <li>2023-10-27<ul> <li>minor formatting</li> </ul> </li> <li>2023-10-15<ul> <li>minor formatting</li> </ul> </li> <li>2023-10-14<ul> <li>initially published</li> </ul> </li> </ul>","tags":["AWS","Python","Software Development"]},{"location":"blog/bbcb98c8-dc68-4cb4-a660-8cf4d7c224ae/#prerequisites","title":"Prerequisites","text":"<p>Before proceeding with the next sections, you must ensure that you have:</p> <ol> <li>An active AWS Account</li> <li>A working installation of AWS CLI v2</li> <li>Configured AWS credentials (see instructions)</li> </ol>","tags":["AWS","Python","Software Development"]},{"location":"blog/bbcb98c8-dc68-4cb4-a660-8cf4d7c224ae/#handler","title":"Handler","text":"<p>To integrate AWS CloudWatch Logs in Python using boto3 and subclassing the <code>Handler</code> class of the standard logging module, you can create a custom CloudWatch Logs handler. Here's a step-by-step guide:</p> <p>Step 1: Install the necessary libraries if you haven't already:</p> <pre><code>pip install boto3 boto3-stubs[logs]\n</code></pre> <p>Step 2: Create a custom CloudWatch Logs handler by subclassing the <code>logging.Handler</code> class. Override the <code>emit</code> method to send log messages to CloudWatch Logs:</p> <p>Tip</p> <p>The snippet below is annotated to explain blocks of code inline.</p> aws_cloudwatch_logs_handler.py<pre><code>import datetime\nimport logging\nimport time\nimport typing\nimport uuid\n\nimport boto3\nfrom mypy_boto3_logs import client, type_defs\n\nLogGroupRetentionDaysType = typing.Optional[\n    typing.Literal[\n        1, 3, 5, 7, 14, 30, 60, 90, 120, 150,\n        180, 365, 400, 545, 731, 1096, 1827,\n        2192, 2557, 2922, 3288, 3653,\n    ]\n]\n\"\"\"\nSee: [PutRetentionPolicy](https://tinyurl.com/5fsu3brf)\n\"\"\"\n\n\nclass CloudWatchLogsHandler(logging.Handler):\n    \"\"\"\n    A custom logging handler based on [cloudwatch](https://pypi.org/project/cloudwatch/)\n    by [labrixdigital](https://github.com/labrixdigital)\n    \"\"\"\n\n    def __init__(\n        self,\n        log_group: str = None,\n        *,\n        log_stream: typing.Optional[str] = None,\n        ttl: LogGroupRetentionDaysType = None,\n        access_key: typing.Optional[str] = None,\n        secret_key: typing.Optional[str] = None,\n        region: typing.Optional[str] = None,\n        profile: typing.Optional[str] = None,\n    ) -&gt; None:\n        self.__log_group = self.__resolve_log_group(log_group)\n        self.__log_stream = self.__resolve_log_stream(log_stream)\n        self.__client = self.__resolve_client(profile, access_key, secret_key, region)\n        self.__next_sequence_token: typing.Optional[str] = None\n        self.__resolve_resources(ttl)\n\n        logging.Handler.__init__(self)\n\n    def __resolve_log_group(self, log_group: str) -&gt; str:\n        if log_group is None:\n            raise ValueError(\"Log Group name is required.\")\n\n        return log_group\n\n    def __resolve_log_stream(self, log_stream: str) -&gt; str:\n        if log_stream is None: \n            return \"stream/{}/{}\".format(\n                datetime.datetime.strftime(datetime.datetime.utcnow(), \"%Y/%m/%d/%H%M\"),\n                str(uuid.uuid4()),\n            )\n\n        return log_stream\n\n    def __resolve_client(\n        self,\n        profile: typing.Optional[str],\n        access_key: typing.Optional[str],\n        secret_key: typing.Optional[str],\n        region: typing.Optional[str],\n    ) -&gt; client.CloudWatchLogsClient:\n        if profile is not None: # (1)!\n            session = boto3.Session(profile_name=profile)\n            return typing.cast(client.CloudWatchLogsClient, session.client(\"logs\"))\n\n        if (access_key is None) and (secret_key is None) and (region is None): # (2)!\n            return typing.cast(client.CloudWatchLogsClient, boto3.client(\"logs\"))\n\n        elif access_key is None: # (3)!\n            return typing.cast(\n                client.CloudWatchLogsClient, boto3.client(\"logs\", region_name=region)\n            )\n\n        else: # (4)!\n            session = boto3.Session(\n                aws_access_key_id=access_key,\n                aws_secret_access_key=secret_key,\n                region_name=region,\n            )\n            return typing.cast(client.CloudWatchLogsClient, session.client(\"logs\"))\n\n    def __resolve_resources(self, ttl: LogGroupRetentionDaysType) -&gt; None:\n        try: # (5)!\n            response: type_defs.DescribeLogStreamsResponseTypeDef = (\n                self.__client.describe_log_streams(logGroupName=self.__log_group)\n            )\n            for each_log_stream in response[\"logStreams\"]:\n                if each_log_stream[\"logStreamName\"] == self.__log_stream:\n                    self.__next_sequence_token = (\n                        each_log_stream[\"uploadSequenceToken\"]\n                        if \"uploadSequenceToken\" in each_log_stream\n                        else None\n                    )\n\n            if self.__next_sequence_token == None: # (6)!\n                self.__client.create_log_stream(\n                    logGroupName=self.__log_group, logStreamName=self.__log_stream\n                )\n\n        except self.__client.exceptions.ResourceNotFoundException: # (7)!\n            self.__client.create_log_group(logGroupName=self.__log_group)\n            self.__client.create_log_stream(\n                logGroupName=self.__log_group, logStreamName=self.__log_stream\n            )\n            if ttl is not None:\n                self.__client.put_retention_policy(\n                    logGroupName=self.__log_group, retentionInDays=ttl\n                )\n\n        except self.__client.exceptions.ResourceAlreadyExistsException: # (8)!\n            pass\n\n    def send_log(self, timestamp: int, message: str) -&gt; None:\n        log_event = type_defs.InputLogEventTypeDef(timestamp=timestamp, message=message)\n\n        if self.__next_sequence_token is not None:\n            response = self.__client.put_log_events(\n                logGroupName=self.__log_group,\n                logStreamName=self.__log_stream,\n                sequenceToken=self.__next_sequence_token,\n                logEvents=[log_event],\n            )\n        else:\n            response = self.__client.put_log_events(\n                logGroupName=self.__log_group,\n                logStreamName=self.__log_stream,\n                logEvents=[log_event],\n            )\n\n        self.__next_sequence_token = response[\"nextSequenceToken\"] # (9)!\n\n    def emit(self, record: logging.LogRecord) -&gt; None:\n        \"\"\"\n        This is the overriden function from the handler to send logs to AWS\n        \"\"\"\n\n        timestamp = round(time.time() * 1000) # (10)!\n        message = self.format(record) # (11)!\n\n        try:\n            self.send_log(timestamp, message)\n\n        except self.__client.exceptions.DataAlreadyAcceptedException as e: # (12)!\n            e_str = str(e)\n            self.__next_sequence_token = e_str[e_str.find(\"sequenceToken: \") + 15 :]\n\n        except self.__client.exceptions.InvalidSequenceTokenException as e: # (13)!\n            e_str = str(e)\n            self.__next_sequence_token = e_str[e_str.find(\"sequenceToken is: \") + 18 :]\n            self.send_log(timestamp, message)\n\n        except self.__client.exceptions.ClientError as e: # (14)!\n            time.sleep(3)\n            self.send_log(timestamp, message)\n</code></pre> <ol> <li>Return <code>client</code> based on provided profile.</li> <li>Return <code>client</code> based on AWS credentials set in config file.</li> <li>Return <code>client</code> based on AWS credentials set in config file and custom region.</li> <li>Return <code>client</code> based on manually entered AWS credentials.</li> <li>Find the sequence token in the log streams.</li> <li>If no sequence token is found, then create the stream.</li> <li>On <code>ResourceNotFoundException</code>, create the log group.</li> <li>On <code>ResourceAlreadyExistsException</code>, ignore.</li> <li>Store the next sequence token.</li> <li>Get the current Unix time in milliseconds (required by AWS).</li> <li>Format the message using the configured <code>Formatter</code>.</li> <li>Ignore <code>DataAlreadyAcceptedException</code> and get next sequence token.</li> <li>If the current sequence token is invalid, change the sequence token and retry.</li> <li>Wait and try resending.</li> </ol>","tags":["AWS","Python","Software Development"]},{"location":"blog/bbcb98c8-dc68-4cb4-a660-8cf4d7c224ae/#usage","title":"Usage","text":"<p>After creating our custom <code>Handler</code>, we can use it the same way we use other built-in logging handlers in Python's logging module:</p> app.py<pre><code>import logging\nimport time\nimport typing\n\nfrom aws_cloudwatch_logs_handler import CloudWatchLogsHandler\n\nLOGGER_NAME: typing.Final[str] = \"LoggerName\"\n\nlogger = logging.getLogger(LOGGER_NAME)\n\nlogger.setLevel(logging.DEBUG)\n\nformatter = logging.Formatter(\n    (\n        \"%(asctime)s.%(msecs)03d\" + time.strftime(\"%z\") + \" \"\n        \"| %(process)-8d \"\n        \"| %(thread)-16d \"\n        \"| %(levelname)-8s \"\n        \"| %(lineno)-4d \"\n        \"| %(message)s\"\n    ),\n    \"%Y-%m-%dT%H:%M:%S\",\n)\n\nhandler = CloudWatchLogsHandler(\"/custom/application-logs\", ttl=1)\n\nhandler.setLevel(logging.DEBUG)\nhandler.setFormatter(formatter)\n\nlogger.addHandler(handler)\n\n# Sample\nlogger.debug(\"Debug message.\")\nlogger.info(\"Info message.\")\nlogger.warning(\"Warning message.\")\nlogger.error(\"Error message.\")\nlogger.critical(\"Warning message.\")\nlogger.exception(\"Exception message.\")\n</code></pre> <p>After running our custom handler:</p> <pre><code>python app.py\n</code></pre> <p>We can now check CloudWatch Logs in our AWS console and we should see our sample log messages:</p> <p> </p> Figure 1: Log Group <code>/custom/application-logs</code> <p> </p> Figure 2: Log Stream <code>stream/YYYY/MM/DD/hhmm/UUID4</code> <p> </p> Figure 3: Log Events","tags":["AWS","Python","Software Development"]},{"location":"blog/bbcb98c8-dc68-4cb4-a660-8cf4d7c224ae/#use-cases","title":"Use Cases","text":"<p>The implementation of CloudWatch Logs in Python using boto3 and a custom CloudWatch Logs handler can be used in various scenarios and use cases. Here's a list of possible use cases for this implementation:</p> <ol> <li>Application Logging: Centralized logging for your Python applications to monitor their behavior, troubleshoot issues, and track application performance.</li> <li>Distributed Systems: In distributed systems, you can aggregate logs from different components and services into a single CloudWatch Logs log group for centralized monitoring.</li> <li>Error Tracking: Use CloudWatch Logs to capture and track application errors, exceptions, and stack traces, making it easier to identify and resolve issues.</li> <li>Security and Compliance: Store security-related logs and audit trails in CloudWatch Logs to help meet compliance requirements and monitor suspicious activities.</li> <li>Application Performance Monitoring (APM): Correlate logs with performance data to gain insights into application performance and pinpoint bottlenecks.</li> </ol>","tags":["AWS","Python","Software Development"]},{"location":"blog/bbcb98c8-dc68-4cb4-a660-8cf4d7c224ae/#conclusion","title":"Conclusion","text":"<p>In conclusion, harnessing the power of AWS CloudWatch Logs in Python by leveraging a custom CloudWatch Logs Handler subclass of Python's logging module offers developers a robust and flexible solution for centralized log management. This implementation empowers Python applications to seamlessly transmit log data to CloudWatch Logs, allowing for real-time monitoring, comprehensive troubleshooting, and a wealth of potential use cases. Whether tracking application performance, debugging issues, or ensuring compliance and security, the integration of CloudWatch Logs through a custom handler equips developers with the essential tools to take control of their log data and optimize their AWS environment.</p>","tags":["AWS","Python","Software Development"]},{"location":"blog/category/aws/","title":"AWS","text":""},{"location":"blog/category/python/","title":"Python","text":""},{"location":"blog/category/software-development/","title":"Software Development","text":""},{"location":"tags/","title":"Tags","text":"<p>Following is a list of relevant tags:</p>"},{"location":"tags/#aws","title":"AWS","text":"<ul> <li>Managing Amazon S3 Buckets and Objects using Boto3</li> <li>Python Logging Part 2: AWS CloudWatch Logs Handler</li> </ul>"},{"location":"tags/#python","title":"Python","text":"<ul> <li>Managing Amazon S3 Buckets and Objects using Boto3</li> <li>Python Logging Part 1: Handlers and Formatters</li> <li>Python Logging Part 2: AWS CloudWatch Logs Handler</li> </ul>"},{"location":"tags/#software-development","title":"Software Development","text":"<ul> <li>Managing Amazon S3 Buckets and Objects using Boto3</li> <li>Python Logging Part 1: Handlers and Formatters</li> <li>Python Logging Part 2: AWS CloudWatch Logs Handler</li> </ul>"}]}